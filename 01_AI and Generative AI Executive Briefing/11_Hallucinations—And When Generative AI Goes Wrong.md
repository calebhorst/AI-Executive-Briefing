## Hallucinationsâ€”And When Generative AI Goes Wrong
```
To get an idea of how a generative A I works and also how it can go wrong. Let's just begin with the autocomplete you get, whenever you write a text message, as I write it, I get these suggestions and the more I write the suggestions change and refine. So if I get to the point where my message begins, it's going to be a beautiful, what's the next word? Well, you could analyze millions of text messages, you could look at previous usage of these words or phrases like this and you would probably find the most likely next word here is day. It's going to be a beautiful day or perhaps night. But it's very important to recognize this messaging app doesn't actually understand anything. It doesn't know my mood, what's going through my mind. It is simply a statistical likelihood that after writing these words, the next word is probably day or night. But the fact it's the most likely option doesn't mean it's correct. I might have just walked into error room decorated for an upcoming wedding reception and I was about to write, it's going to be a beautiful wedding. Could have been 1000 other things beautiful cake, beautiful house, beautiful painting. But we can understand why the auto complete suggested beautiful day. It is the option with the highest probability, but it can still be wrong. OK. You might think what does that have to do with generative A I? Well, because in a way, a lot of generative A I works by trying to be this tremendously complex autocomplete. Given the fact that you just wrote all these words here is the statistically most likely next word or phrase or entire paragraph. And as with autocomplete, with generative A I, you can often get a suggestion where you kind of understand why you got it and it's still completely wrong. However, what can be a real problem is when you use generative A I as any kind of research assistant for something where you're not an expert, it can give you the wrong result and you don't know it's wrong because the results you get from generative A I are often so impressive. It can be very easy to assume the computer just knows what it's doing. For example, let's say I use an image generating A I like Dali or mid journey. And I give it a prompt that describes a fairly unique combination of objects and different qualities I want from this new image. I want a cinematic photograph of a golden skull on top of an old oak table in a library. And a few seconds later, I get several options of exactly what I was asking for. The A I has gone into its vast amount of training information, all the different examples of skulls and table and library. It knows what golden things look like. It knows about the qualities of a photograph as opposed to an oil painting or a pencil sketch. It knows things described as cinematic would have out of focus backgrounds. We get very impressive results. But if I give it a prompt, that might even at first glance seem simpler but requires a little deeper understanding, we can very quickly start to test the edges of this. So I'm going to use a prompt here and see if an image comes to mind. As I say, I want a photo of a boy looking at an egg, he's just dropped on a concrete floor and here's the results I get. Now they do look really good. But any human artist would have thought, well, what happens when you drop an egg on a concrete floor, it's going to break. But the a IDE doesn't do that because these generative A is do not have a fundamental knowledge of the world. They don't understand consequences, they don't understand true and false. They are amazing at matching patterns and making new combinations. But that doesn't mean they actually understand anything. Now if you're thinking Yeah. Yeah. Yeah. But I don't really need to generate these kinds of image. Well know that this same issue also applies with text based generative A, I like Chat GP T and Google Bod these A is all do their best to generate new results that are convincing and believable. It doesn't mean the results they generate are actually correct. There's even a term in generative A I called a hallucination where the A I basically makes up the answer. It might sound right. It's sometimes incredibly convincing, but it's not true. Here's one simple example, I was recently giving a demo of chat GP T to a friend who works in marketing. So I asked chat GP T Yeah. OK. Write a blog post about the metaverse and talk about new hardware. And it did, in fact, it wrote a pretty good article about the metaverse what's going on. And then it told me about the new Dream View. Xr 2023 headset. It gave me specific about it eight K resolution and its 220 degree field of view. There's no such device it doesn't exist. This is a hallucination. Now, what it generated is very close to being right? An article about the metaverse that described new hardware would be very much like this text. It would be, let's say 95 98% like this. It would just talk about a headset that actually existed. You see, while generative A I can give you amazing results. We're still at the point where you should fact check anything you get from it, particularly if you're using them as any kind of technical reference or technical explanation. Generative A I is fantastic, but it does bring new issues, not just those hallucinations, but the potential impact of misinformation, either intentional or unintentional along with issues around data privacy, content ownership and many larger issues about the impact this will have on many professions. So let's finish by talking specifically about some of those current and expected future problems with A I.
```

## Notes
- **Autocomplete Illustration:** Autocomplete in text messages demonstrates a statistical likelihood for the next word but lacks true understanding of context, akin to generative AI.
- **Generative AI and Autocomplete Comparison:** Generative AI operates similarly to a complex autocomplete, predicting the most probable next word or paragraph based on preceding text.
- **Potential Issues:** Generative AI's impressive results can mislead users into assuming accuracy, leading to potential errors, especially when applied in areas outside the user's expertise.
- **Visual AI Example:** Image-generating AI excels at creating impressive visuals based on specific prompts, yet lacks fundamental knowledge, leading to inconsistencies like an unbroken egg on a concrete floor despite the prompt's implications.
- **Text-based AI Issues:** Text-based generative AI, like Chat GPT and Google Bard, might generate convincing yet factually incorrect content, termed "hallucination," demonstrated by a fictitious hardware description in an article about the metaverse.
- **Caution with AI Outputs:** Users must fact-check AI-generated content, particularly when used for technical or factual reference, due to the potential for misinformation or inaccuracies.
- **Broader AI Concerns:** Generative AI presents concerns regarding misinformation, data privacy, content ownership, and its impact on various professions, indicating emerging challenges in the AI landscape.
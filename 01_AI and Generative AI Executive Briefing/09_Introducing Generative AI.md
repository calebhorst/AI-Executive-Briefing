## Introducing Generative AI
```
I've talked about several different categories of A I. But the one that's really gotten the world's attention and has had a monumental impact in a remarkably short amount of time is generative A I. Now this as the name might suggest is A I that is designed to generate content and not from some pre scripted, predefined set of options but to create entirely new content. Some of these A is create written text, others can generate images or computer code, even video or music. And they work by first being trained on massive massive amounts of data, whether that's text or images or code. So a text based generative A I will have analyzed millions even billions of text sources, web pages, books, articles, scientific papers, an image based generative A I will have been trained on countless images, photographs, illustrations, diagrams and the combination of all the data they've been trained on together with your suggestions, your prompts allows them to then generate that brand new content. The most well known of these is Chat GP T from the company Open A I chat GP T released in November 2022. They hit a million users within five days, 100 million users within two months, just incredible uptake in adoption and a lot of other tech companies suddenly scrambling to play. Catch up. In 2023 Google released their generative A I called Bard and Microsoft also released a version of their Bing search engine that added generative A I features into the search experience. Now behind the scenes, the new Microsoft bing is actually using a version of the same A I that chat GP T uses. But the implementation of it is different, you don't get identical results. Now, the three that I've just mentioned are all text based. They all use an A I technique called a large language model or LLM. And they use the deep learning and neural networks we talked about earlier, they're trained on staggering amounts of text content. They're very time consuming and very expensive to train. But once they're trained, large language models can generate text that is genuinely difficult to distinguish from text written by humans. Having said that not all generative A I uses large language models because we have other options that are image based or audio or video. The better known ones here include dial E which is also from open A I and it generates images based on prompts. We also have options like mid journey and stable diffusion. Now with these, we don't use the term large language model. You may see the term foundation model used to describe them, which is a slightly more generic term. So all the large language models can be considered a type of foundation model. But foundation models describe other modalities like images and video as well. Now the text based generative A I and the image based generative A I might seem like entirely different technologies but they error fundamentally very similar. And I believe the image generation tools can actually be more useful and helpful to understand why and where generative A I gets things wrong. Because with all of them, whether they generate text or generate images or audio, they can be extremely good at it, they can give you incredible results and they can be completely wrong all at the same time. And for the same reason, let's talk about why.
```

## Notes
Absolutely! Here's a breakdown of Generative AI:

**Generative AI Overview:**

- **Definition:** AI designed to produce original content without predefined options.
- **Categories:**
  - Text-based AI generates written content.
  - Image-based AI creates images from vast training data sets.
  - Capable of generating computer code, video, and music as well.

- **Prominent Generative AI Models:**
  - **ChatGPT:** OpenAI's model, released in 2022, rapidly gained popularity with a million users in five days and 100 million in two months.
  - **Google's Bard and Microsoft's Bing:** Released as text-based generative AI tools, using similar AI techniques to ChatGPT but implemented differently.

- **Large Language Models (LLMs):**
  - Utilize deep learning and neural networks, trained on extensive text content, requiring substantial time and expense.
  - Capable of generating text that closely resembles human-authored content.

- **Diversity in Generative AI:**
  - Not limited to text; includes image-based models like DALLÂ·E for generating images based on prompts.
  - Also involves models for audio and video generation like MidJourney and Stable Diffusion, termed foundation models.

- **Similarities Across Modalities:**
  - Despite different content outputs (text, images, etc.), underlying technologies share fundamental principles.

- **Understanding Generative AI Errors:**
  - Although capable of impressive results, Generative AI can also produce incorrect or flawed content.
  - Examining image generation tools can aid in comprehending where and why Generative AI fails, as they can demonstrate errors more visibly.
  
- **Challenges and Errors:**
  - Generative AI can produce incredible outcomes but also glaring inaccuracies, highlighting the complexity and limitations in these systems.